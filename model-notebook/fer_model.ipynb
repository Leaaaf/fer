{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Face Emotions Recognise\n",
        "#### Python notebook used to generate and test the final CNN used by app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJE4d1Hjxdc2"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, BatchNormalization, MaxPooling2D\n",
        "from keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### GradCAM class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GradCAM:\n",
        "    def __init__(self, model, classIdx, layerName=None):\n",
        "        self.model = model\n",
        "        self.classIdx = classIdx\n",
        "        self.layerName = layerName\n",
        "        if self.layerName is None:\n",
        "            self.layerName = self.find_target_layer()\n",
        "            \n",
        "    def find_target_layer(self):\n",
        "        for layer in reversed(self.model.layers):\n",
        "            if len(layer.output_shape) == 4:\n",
        "                return layer.name\n",
        "        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n",
        "        \n",
        "    def compute_heatmap(self, image, eps=1e-8):\n",
        "        gradModel = Model(\n",
        "            inputs=[self.model.inputs],\n",
        "            outputs=[self.model.get_layer(self.layerName).output,self.model.output]\n",
        "       )\n",
        "           \n",
        "        with tf.GradientTape() as tape:\n",
        "            inputs = tf.cast(image, tf.float32)\n",
        "            (convOutputs, predictions) = gradModel(inputs)\n",
        "            loss = predictions[:, self.classIdx]\n",
        "            grads = tape.gradient(loss, convOutputs)\n",
        "\n",
        "            castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
        "            castGrads = tf.cast(grads > 0, \"float32\")\n",
        "            guidedGrads = castConvOutputs * castGrads * grads\n",
        "            convOutputs = convOutputs[0]\n",
        "            guidedGrads = guidedGrads[0]\n",
        "\n",
        "            weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
        "            cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
        "\n",
        "            (w, h) = (image.shape[2], image.shape[1])\n",
        "            heatmap = cv2.resize(cam.numpy(), (w, h))\n",
        "            numer = heatmap - np.min(heatmap)\n",
        "            denom = (heatmap.max() - heatmap.min()) + eps\n",
        "            heatmap = numer / denom\n",
        "            heatmap = (heatmap * 255).astype(\"uint8\")\n",
        "        return heatmap\n",
        "\n",
        "    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n",
        "        colormap = cv2.COLORMAP_VIRIDIS):\n",
        "        heatmap = cv2.applyColorMap(heatmap, colormap)\n",
        "        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n",
        "        return (heatmap, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load dataset, explore and split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG01cTyxy4l3"
      },
      "outputs": [],
      "source": [
        "path_dataset = './datasets/fer2013.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "8ccN2Ft71ekL",
        "outputId": "501ef475-3511-40f7-f60c-5860a2e2cd33"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(path_dataset)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show the size of dataset and the number of records per emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"The size of dataset is {len(data)}\")\n",
        "print(f\"The number of records per emotion:\\n {data['emotion'].value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Map all the emotions in the dataset with real values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGIZ1sBM1xlw"
      },
      "outputs": [],
      "source": [
        "emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXU4dXKy4Kei"
      },
      "source": [
        "#### Split dataset into Train, Validation and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data(data):\n",
        "    image_array = np.zeros(shape=(len(data), 48, 48,1))\n",
        "    image_label = np.array(list(map(int, data['emotion'])))\n",
        "    \n",
        "    for i, row in enumerate(data.index):\n",
        "        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n",
        "        image = np.reshape(image, (48, 48,1))\n",
        "        image = image / 255.0\n",
        "        image_array[i] = image\n",
        "        \n",
        "    return image_array, image_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhAKirhKlD94"
      },
      "outputs": [],
      "source": [
        "full_train_images, full_train_labels = prepare_data(data[data['Usage']=='Training'])\n",
        "test_images, test_labels = prepare_data(data[data['Usage']!='Training'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD5I8DnUlHYY"
      },
      "outputs": [],
      "source": [
        "train_images, valid_images, train_labels, valid_labels = train_test_split(full_train_images, full_train_labels, test_size=0.2, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model - declaration, fit, evaluation and export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwTBJsSd7KSV"
      },
      "outputs": [],
      "source": [
        "input_reshape = (48, 48, 1)\n",
        "\n",
        "strides = (2, 2)\n",
        "pool_size = (2, 2)\n",
        "kernel_size = (3, 3)\n",
        "\n",
        "epochs = 30\n",
        "batch_size = 256\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=kernel_size, activation='relu', padding='same', input_shape=input_reshape))\n",
        "model.add(MaxPooling2D(pool_size=pool_size, strides=strides))\n",
        "model.add(Conv2D(128, kernel_size=kernel_size, activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size, strides=strides))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(7, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary of model declared in previous step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkfuKMY8Ioyf",
        "outputId": "e9b3736c-d4ec-4154-eb94-53ecd15aa811"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fit model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit model using Adam as Optimizer and compile that with loss = `sparse_categorical_crossentropy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibWwn5UwQoNV"
      },
      "outputs": [],
      "source": [
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YEx0GscDl53p",
        "outputId": "45ff71dd-8c30-42ce-97ef-3dcfe7bc2cfc"
      },
      "outputs": [],
      "source": [
        "h1 = model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, verbose=1, validation_data =(valid_images, valid_labels)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Show results of CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definition of functions used to show result achived by the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function used to plot the Train Accuracy compared to Validation Accuracy and the Training Loss compared to Validation Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_model_history(history):\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.subplot(2, 1, 1)\n",
        "  plt.plot(acc, label='Training Accuracy')\n",
        "  plt.plot(val_acc, label='Validation Accuracy')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.ylim([min(plt.ylim()),1])\n",
        "  plt.title('Training and Validation Accuracy')\n",
        "\n",
        "  plt.subplot(2, 1, 2)\n",
        "  plt.plot(loss, label='Training Loss')\n",
        "  plt.plot(val_loss, label='Validation Loss')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.ylabel('Cross Entropy')\n",
        "  plt.ylim([0,2.0])\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function used to show the confusion matrix of model and evaluate its overall accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_cf_matrix():\n",
        "  test_prob = model.predict(test_images)\n",
        "  test_pred = np.argmax(test_prob, axis=1)\n",
        "  test_accuracy = np.mean(test_pred == test_labels)\n",
        "\n",
        "  print(test_accuracy)\n",
        "\n",
        "  conf_mat = confusion_matrix(test_labels, test_pred)\n",
        "\n",
        "  pd.DataFrame(conf_mat, columns=emotions.values(), index=emotions.values())\n",
        "\n",
        "  fig, ax = plot_confusion_matrix(conf_mat=conf_mat,\n",
        "                                  show_normed=True,\n",
        "                                  show_absolute=False,\n",
        "                                  figsize=(8, 8))\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function used to show the activation region, on image, used by the CNN to make the prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_color_map():\n",
        "  plt.figure(figsize=[16,16])\n",
        "  for i in range(36):\n",
        "    img = test_images[i,:,:,0]\n",
        "    p_dist = model.predict(img.reshape(1,48,48,1))\n",
        "    k = np.argmax(p_dist)\n",
        "    p = np.max(p_dist)\n",
        "\n",
        "    cam = GradCAM(model, k)\n",
        "    heatmap = cam.compute_heatmap(img.reshape(1,48,48,1))\n",
        "\n",
        "    plt.subplot(6,6,i+1)\n",
        "    plt.imshow(img, cmap='binary_r')\n",
        "    plt.imshow(heatmap, alpha=0.5, cmap='coolwarm')\n",
        "    plt.title(f'{emotions[test_labels[i]]} - ({emotions[k]} - {p:.4f})')\n",
        "    plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TypcZ61r0-qL",
        "outputId": "f0f6763f-7abe-4f36-a936-832611bd2afd"
      },
      "outputs": [],
      "source": [
        "plot_model_history(h1)\n",
        "print_cf_matrix()\n",
        "print_color_map()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhfdGx15TZtU"
      },
      "source": [
        "#### Export model to JSON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we need to export model and his weights in json format. This allow the final React Native application to used it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt16ifumXMyF"
      },
      "outputs": [],
      "source": [
        "import tensorflowjs as tfjs\n",
        "tfjs.converters.save_keras_model(model, 'models')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test model with webcam or uploaded image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Take a photo from webcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5uIlmAT0d9R"
      },
      "outputs": [],
      "source": [
        "from base64 import b64decode\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "def take_photo(filename='photo.jpeg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Analyze photo taken by webcam or uploaded manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsb4cUombJ5W"
      },
      "outputs": [],
      "source": [
        "def emotion_analysis(emotions):\n",
        "    objects = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
        "    y_pos = np.arange(len(objects))\n",
        "    plt.bar(y_pos, emotions, align='center', alpha=0.9)\n",
        "    plt.tick_params(axis='x', which='both', pad=10,width=4,length=10)\n",
        "    plt.xticks(y_pos, objects)\n",
        "    plt.ylabel('percentage')\n",
        "    plt.title('emotion')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw8EwMFzbCzc"
      },
      "outputs": [],
      "source": [
        "def predict_input(images):\n",
        "    predictions = model.predict(images)\n",
        "    for p in predictions:\n",
        "      emotion_analysis(p)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4C76kDKa1AY"
      },
      "outputs": [],
      "source": [
        "def predict_from_webcam(x):\n",
        "  new_image_array = np.zeros(shape=(1,48,48))\n",
        "  x = np.reshape(x,(48,48))\n",
        "  new_image_array[0] = x\n",
        "  new_predict_images = new_image_array.reshape((new_image_array.shape[0], 48, 48, 1))\n",
        "  #arr_to_predict = new_image_array.astype('float32')/255\n",
        "  predict_input(new_predict_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fNthaXxmS4oj",
        "outputId": "61c1f14f-c9fe-47bc-e945-a9c54132b743"
      },
      "outputs": [],
      "source": [
        "path = './photo.jpeg'\n",
        "image = cv2.imread(path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "face_cascade = cv2.CascadeClassifier('./extlib/haarcascade_frontalface_default.xml')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "# Detect faces\n",
        "faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "for (x, y, w, h) in faces:\n",
        "  cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "  img = image[y : y+w, x:x+w]\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "  img = keras.applications.mobilenet_v2.preprocess_input(img)\n",
        "  img = cv2.resize(img,(48,48))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "  predict_from_webcam(img)\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Sistemi Digitali M Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
